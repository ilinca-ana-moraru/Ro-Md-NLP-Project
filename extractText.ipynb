{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from selenium import webdriver\n",
    "import os\n",
    "from googlesearch import search\n",
    "import time\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# pip install requests\n",
    "# pip install beautifulsoup4\n",
    "# pip install pandas\n",
    "# pip install langdetect\n",
    "# pip install google\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHtmlTags(htmlText):\n",
    "    soup = BeautifulSoup(htmlText, 'html.parser')\n",
    "    for htmlText in soup.find_all(['header', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        htmlText.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    words = text.split()\n",
    "    # Filter out words containing \"https://\"\n",
    "    filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "        # visibleText = soup.get_text()\n",
    "        body_tag = soup.body\n",
    "        if body_tag:\n",
    "                visibleText = body_tag.get_text(separator='\\n', strip=True)\n",
    "                visibleText = removeHtmlTags(visibleText)\n",
    "                lines = visibleText.split('\\n')\n",
    "                # i check for '.' and for a line longer that 200 chars to reduce titles, headers\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0 and len(line) > 200 and '.' in line)\n",
    "                return text\n",
    "        else:\n",
    "            print(\"Body tag not found.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def findLinks(url, sitesToVisit, visitedSites):\n",
    "    base_domain = urlparse(url).netloc.lower()\n",
    "    blacklist = ['youtube.com', 'instagram.com', 'pinterest.com', 'twitter.com', 'facebook.com',\n",
    "                 'login', 'cookie', 'cookies', 'politica-de-confidentialitate', 'despre-noi', \n",
    "                 'termeni-si-conditii', 'contact', 'privacy-policy', 'search', 'archive', \n",
    "                 'tag', 'category', 'forum', 'login', 'register', 'profile', 'logout', \n",
    "                 'sign-up','log-in','my-account','privacy','conditii','service','terms','comment','comentariu','respond',\n",
    "                 'conditions','about','sitemap','cont','comments','feed','politica-editoriala','cum-ne-poti-ajuta',\n",
    "                 'password','paywall','arhiva','archive','termeni','despre','admin','newsletter',\n",
    "                 'cart', 'checkout', 'shop', 'store', 'download', 'subscribe', 'unsubscribe','produs','abonare',\n",
    "                 'terms-of-service', 'about-us', 'faq', 'donate', 'events', 'calendar', \n",
    "                 'faq', 'gallery', 'help', 'guidelines', 'policy']\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        navbar = soup.find('nav', class_='navbar')\n",
    "        footer = soup.find('footer', class_='footer')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            if navbar and link in navbar.find_all('a') or footer and link in footer.find_all('a'):\n",
    "                continue\n",
    "            href = link['href']\n",
    "            if href.startswith('#') or href.startswith('javascript:'):\n",
    "                continue  \n",
    "            currentLink = urljoin(url, href)\n",
    "            current_domain = urlparse(currentLink).netloc.lower()\n",
    "            if current_domain == base_domain:\n",
    "                if currentLink not in visitedSites and currentLink not in sitesToVisit and all(b not in currentLink.lower() for b in blacklist):\n",
    "                    sitesToVisit.append(currentLink)\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def extractAll(url, file_name,siteIdx, nrOfSitesToVisit = 100):\n",
    "    data = pd.DataFrame(columns=['siteIdx','link','text'])\n",
    "    sitesToVisit = []\n",
    "    visitedSites = []\n",
    "    initialSitesToVisit = nrOfSitesToVisit\n",
    "    tries = 1\n",
    "    findLinksCode = findLinks(url, sitesToVisit, visitedSites)\n",
    "    startTime = time.time()\n",
    "    if findLinksCode == 0:\n",
    "        while nrOfSitesToVisit:\n",
    "            if sitesToVisit:\n",
    "                link = sitesToVisit[-1]\n",
    "                text = extractText(link)\n",
    "                if text:\n",
    "                    #succesful text extraction\n",
    "                    new_row = {'siteIdx':siteIdx, 'link': link, 'text': text}\n",
    "                    data = pd.concat([data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                    nrOfSitesToVisit -= 1\n",
    "                    startTime = time.time()\n",
    "                    # print(\"Succesfull visited site: \",link)\n",
    "\n",
    "                visitedSites.append(link)\n",
    "                sitesToVisit.pop()\n",
    "                # print(\"Visited site: \",link)\n",
    "            elif nrOfSitesToVisit:\n",
    "                if len(visitedSites) > tries: \n",
    "                    findLinksCode = findLinks(visitedSites[tries], sitesToVisit, visitedSites)\n",
    "                    tries += 1\n",
    "                else:\n",
    "                    print('Only', initialSitesToVisit - nrOfSitesToVisit, 'articles out of', initialSitesToVisit, 'for', url)\n",
    "                    nrOfSitesToVisit = 0\n",
    "            endTime = time.time()\n",
    "            if endTime - startTime > 300:\n",
    "                print(\"3 minute time limit exceeded, \",tries,\" unsuccesful tries\")\n",
    "                if not os.path.isfile(file_name):\n",
    "                    data.to_csv(file_name, index=False)\n",
    "                else:\n",
    "                    data.to_csv(file_name, index=False, mode='a', header=False)\n",
    "                return data\n",
    "    if not os.path.isfile(file_name):\n",
    "        data.to_csv(file_name, index=False)\n",
    "    else:\n",
    "        data.to_csv(file_name, index=False, mode='a', header=False)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def processUrls(nrOfSitesToVisit,urls, fileName):\n",
    "\n",
    "    with open(fileName, 'w'):\n",
    "        pass\n",
    "\n",
    "    allData = pd.DataFrame(columns=['siteIdx','link', 'Text'])\n",
    "    siteIdx = 0\n",
    "    for url in urls:\n",
    "        try:\n",
    "            siteIdx += 1\n",
    "            data = extractAll(nrOfSitesToVisit, url,fileName,siteIdx)\n",
    "            allData = pd.concat([allData, data], ignore_index=True) \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while processing {url}: {e}\")\n",
    "    return allData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "agricultureBlogsMd = [\n",
    "\"https://agroexpert.md/\",\n",
    "\"https://agrobiznes.md/\",\n",
    "\"https://agrotv.md/category/stiri/agricultura/\",\n",
    "\"https://md.agrointel.ro/category/fermier-in-republica-moldova\",\n",
    "\"https://agricolahub.md/blog/\"\n",
    "]\n",
    "\n",
    "fileName = \"agricultureMd.csv\"\n",
    "with open(fileName, 'w'):\n",
    "    pass\n",
    "\n",
    "agricultureDataMd = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "# agricultureDataMd = pd.concat([agricultureDataMd, ...], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agroexpert\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "        for strong_tag in soup.find_all('strong'):\n",
    "            strong_tag.extract()\n",
    "\n",
    "        for fulger_tag in soup.find_all('p', class_=\"p-socials\"):\n",
    "            fulger_tag.extract()\n",
    "\n",
    "        content_div = soup.find('div', class_='content')\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 1\n",
    "url = agricultureBlogsMd[blogIdx-1]\n",
    "agroexpertData = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilinc\\AppData\\Local\\Temp\\ipykernel_2076\\4017643648.py:14: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(extracted_content, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "#agrobiznes\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        meta_tag = soup.find('meta', property=\"og:description\")\n",
    "\n",
    "        if meta_tag:\n",
    "            extracted_content = meta_tag['content']\n",
    "            \n",
    "            if extracted_content:\n",
    "                soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "                text = soup.get_text(separator=\" \", strip=True)\n",
    "                words = text.split()\n",
    "                # Filter out words containing \"https://\"\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "    \n",
    "blogIdx = 2\n",
    "url = agricultureBlogsMd[blogIdx-1]\n",
    "agrobiznesData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 minute time limit exceeded,  140  unsuccesful tries\n"
     ]
    }
   ],
   "source": [
    "#agrotv\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_='entry-content')\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 3\n",
    "url = agricultureBlogsMd[blogIdx-1]\n",
    "agrotvData = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agrointel\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        for address_tag in soup.find_all('address'):\n",
    "            address_tag.extract()\n",
    "\n",
    "        for h1_tag in soup.find_all('h1'):\n",
    "            h1_tag.extract()\n",
    "\n",
    "        for ads_tag in soup.find_all('ins', class_=\"adsbygoogle\"):\n",
    "            ads_tag.extract()\n",
    "\n",
    "        content_div = soup.find('article')\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 4\n",
    "url = agricultureBlogsMd[blogIdx-1]\n",
    "agrointelData = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agricolahub\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='ty-mainbox-body')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "    \n",
    "blogIdx = 5\n",
    "url = agricultureBlogsMd[blogIdx-1]\n",
    "agricolahubData = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "agricultureDataMd = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "agricultureDataMd = pd.concat([agricultureDataMd, agroexpertData], ignore_index=True) \n",
    "agricultureDataMd = pd.concat([agricultureDataMd, agrobiznesData], ignore_index=True) \n",
    "agricultureDataMd = pd.concat([agricultureDataMd, agrotvData], ignore_index=True) \n",
    "agricultureDataMd = pd.concat([agricultureDataMd, agrointelData], ignore_index=True) \n",
    "agricultureDataMd = pd.concat([agricultureDataMd, agricolahubData], ignore_index=True) \n",
    "\n",
    "agricultureDataMd.to_csv(fileName, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################RO######AGRICULTURE##############################################\n",
    "agricultureBlogsRo = [\n",
    "\"https://agrointel.ro/category/stiri-agricole\",\n",
    "\"https://blog.magazialucostica.ro/\",\n",
    "\"https://www.stiriagricole.ro/\",\n",
    "\"https://www.agroinfo.ro/\",\n",
    "\"https://agropress.ro/\"\n",
    "\n",
    "]\n",
    "\n",
    "fileName = \"agricultureRo.csv\"\n",
    "with open(fileName, 'w'):\n",
    "    pass\n",
    "\n",
    "agricultureDataRo = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "# agricultureDataMd = pd.concat([agricultureDataMd, ...], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agrointel\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        for address_tag in soup.find_all('address'):\n",
    "            address_tag.extract()\n",
    "\n",
    "        for h1_tag in soup.find_all('h1'):\n",
    "            h1_tag.extract()\n",
    "\n",
    "        for ads_tag in soup.find_all('ins', class_=\"adsbygoogle\"):\n",
    "            ads_tag.extract()\n",
    "\n",
    "        content_div = soup.find('article')\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 1\n",
    "url = agricultureBlogsRo[blogIdx-1]\n",
    "agrointelRoData = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#magazialucostica\n",
    "\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('article')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "    \n",
    "blogIdx = 2\n",
    "url = agricultureBlogsRo[blogIdx-1]\n",
    "magazialucosticaData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stiriagricole\n",
    "\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='elementor-element-3e16f961')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "    \n",
    "blogIdx = 3\n",
    "url = agricultureBlogsRo[blogIdx-1]\n",
    "stiriAgricoleData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 minute time limit exceeded,  18  unsuccesful tries\n"
     ]
    }
   ],
   "source": [
    "#agroinfo\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "\n",
    "        content_div = soup.find('div', id='news_body')\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 4\n",
    "url = agricultureBlogsRo[blogIdx-1]\n",
    "agroinfoRoData = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agropress\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "\n",
    "        content_div = soup.find('div', class_='td-post-content')\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "    \n",
    "blogIdx = 5\n",
    "url = agricultureBlogsRo[blogIdx-1]\n",
    "agropressData = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "agricultureDataRo = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "agricultureDataRo = pd.concat([agricultureDataRo, agrointelRoData], ignore_index=True) \n",
    "agricultureDataRo = pd.concat([agricultureDataRo, magazialucosticaData], ignore_index=True) \n",
    "agricultureDataRo = pd.concat([agricultureDataRo, stiriAgricoleData], ignore_index=True) \n",
    "agricultureDataRo = pd.concat([agricultureDataRo, agroinfoRoData], ignore_index=True) \n",
    "agricultureDataRo = pd.concat([agricultureDataRo, agropressData], ignore_index=True) \n",
    "\n",
    "agricultureDataRo.to_csv(fileName, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "culturalBlogsMd = [\n",
    "\"https://nataalbot.md/category/evenimente-speciale/\",\n",
    "\"https://eucitesc.md/blog-full-width/page/61/\",\n",
    "\"https://teosunny.wordpress.com/\",\n",
    "\"http://www.vitalie-vovc.com/\",\n",
    "\"https://ganduridespletite.com/\"\n",
    "]\n",
    "\n",
    "fileName = \"culturalMd.csv\"\n",
    "with open(fileName, 'w'):\n",
    "    pass\n",
    "\n",
    "# cultureDataRo = pd.DataFrame(columns=['siteIdx','link', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#natal bot\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='post-entry')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "    \n",
    "blogIdx = 1\n",
    "url = culturalBlogsMd[blogIdx-1]\n",
    "natalBotData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eucitesc\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='single-entry-summary-post-content')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "    \n",
    "      \n",
    "    \n",
    "blogIdx = 2\n",
    "url = culturalBlogsMd[blogIdx-1]\n",
    "eucitescData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teosunny\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "        \n",
    "        for tag in soup.find_all('div', class_='p-tag'):\n",
    "            tag.extract()\n",
    "\n",
    "        for tag in soup.find_all('div', class_='sharedaddy'):\n",
    "            tag.extract()\n",
    "\n",
    "        content_div = soup.find('div', class_='p-con')\n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 3\n",
    "url = culturalBlogsMd[blogIdx-1]\n",
    "teosunnyData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 62 articles out of 100 for http://www.vitalie-vovc.com/\n"
     ]
    }
   ],
   "source": [
    "#vitalie\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "        \n",
    "        for tag in soup.find_all('div', class_='p-tag'):\n",
    "            tag.extract()\n",
    "\n",
    "        content_div = soup.find('div', class_='contenuArticle')\n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 4\n",
    "url = culturalBlogsMd[blogIdx-1]\n",
    "vitalieData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ganduri despletite\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "        \n",
    "        div = soup.find('div', class_='entry-content')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "blogIdx = 5\n",
    "url = culturalBlogsMd[blogIdx-1]\n",
    "ganduriDespletiteData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "culturalDataMd = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "culturalDataMd = pd.concat([culturalDataMd, natalBotData], ignore_index=True) \n",
    "culturalDataMd = pd.concat([culturalDataMd, eucitescData], ignore_index=True) \n",
    "culturalDataMd = pd.concat([culturalDataMd, teosunnyData], ignore_index=True) \n",
    "culturalDataMd = pd.concat([culturalDataMd, vitalieData], ignore_index=True) \n",
    "culturalDataMd = pd.concat([culturalDataMd, ganduriDespletiteData], ignore_index=True) \n",
    "\n",
    "culturalDataMd.to_csv(fileName, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "culturalBlogsRo = [\n",
    "\"https://lecturile-emei.blogspot.com/\",\n",
    "\"https://jurnalul-unei-cititoare.ro/blog\",\n",
    "\"https://blogdecititori.ro\",\n",
    "\"https://www.inefabil.ro/\",\n",
    "\"https://booknation.ro/\",\n",
    "\n",
    " ]\n",
    "\n",
    "fileName = \"culturalRo.csv\"\n",
    "with open(fileName, 'w'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lecturileEmei\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        for tr_div in soup.find_all ('tr'):\n",
    "            tr_div.extract()\n",
    "        \n",
    "        div = soup.find('div', class_='entry-content')\n",
    "    \n",
    "        text = div.get_text(separator=\" \", strip=True)\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "        filtered_text = ' '.join(filtered_words)\n",
    "        lines = filtered_text.split('\\n')\n",
    "        text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "        \n",
    "        if len(text) > 1000 and detect(text) == 'ro':\n",
    "            return text\n",
    "      \n",
    "blogIdx = 1\n",
    "url = culturalBlogsRo[blogIdx-1]\n",
    "lecturileemeiData = extractAll(url,fileName,blogIdx,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jurnalul\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        for h_div in soup.find_all ('h1'):\n",
    "            h_div.extract()\n",
    "        \n",
    "        for h_div in soup.find_all ('h2'):\n",
    "            h_div.extract()\n",
    "\n",
    "        for h_div in soup.find_all ('h3'):\n",
    "            h_div.extract()\n",
    "\n",
    "        for h_div in soup.find_all ('h4'):\n",
    "            h_div.extract()\n",
    "\n",
    "        content_div = soup.find_all('div',class_='sqs-html-content')\n",
    "\n",
    "        if content_div:\n",
    "            extracted_paragraphs = []\n",
    "            \n",
    "            for p_tag in content_div:\n",
    "                paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                extracted_paragraphs.append(paragraph_text)\n",
    "            \n",
    "            text = ' '.join(extracted_paragraphs)\n",
    "            \n",
    "            words = text.split()\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            \n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "blogIdx = 2\n",
    "url = culturalBlogsRo[blogIdx-1]\n",
    "jurnalulData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blog de cititori\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        for script_tag in soup.find_all('script'):\n",
    "            script_tag.extract()\n",
    "\n",
    "        for script_tag in soup.find_all('div', class_=\"comments_intro\"):\n",
    "            script_tag.extract()\n",
    "\n",
    "        \n",
    "        for script_tag in soup.find_all('div', class_=\"commentmetadata\"):\n",
    "            script_tag.extract()\n",
    "\n",
    "        for script_tag in soup.find_all(\"div\", id=\"commentsAdd\"):\n",
    "            script_tag.extract()\n",
    "\n",
    "        div = soup.find('article')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "      \n",
    "blogIdx = 3\n",
    "url = culturalBlogsRo[blogIdx-1]\n",
    "blogDeCititoriData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 10 articles out of 100 for https://www.inefabil.ro/\n"
     ]
    }
   ],
   "source": [
    "#inefabil\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        for img_tag in soup.find_all(\"span\", class_=\"rt-reading-time\"):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div', class_=\"post-entry\")\n",
    "        if div:\n",
    "            content_div = div.find_all(\"span\")\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "      \n",
    "blogIdx = 4\n",
    "url = culturalBlogsRo[blogIdx-1]\n",
    "inefabilData = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#booknation\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "        \n",
    "        content_div = soup.find('div', class_='et_pb_post_content_0_tb_body')\n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "      \n",
    "      \n",
    "blogIdx = 5\n",
    "url = culturalBlogsRo[blogIdx-1]\n",
    "booknation = extractAll(url,fileName,blogIdx,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "culturalDataRo = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "culturalDataRo = pd.concat([culturalDataRo, lecturileemeiData], ignore_index=True) \n",
    "culturalDataRo = pd.concat([culturalDataRo, jurnalulData], ignore_index=True) \n",
    "culturalDataRo = pd.concat([culturalDataRo, blogDeCititoriData], ignore_index=True) \n",
    "culturalDataRo = pd.concat([culturalDataRo, inefabilData], ignore_index=True) \n",
    "culturalDataRo = pd.concat([culturalDataRo, booknation], ignore_index=True) \n",
    "\n",
    "culturalDataRo.to_csv(fileName, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentingBlogsMd = [\n",
    "\"https://www.planetamami.com/\",\n",
    "\"https://copilariamamicilor.wordpress.com/\",\n",
    "\"https://jurnaldetatic.blogspot.com/\",\n",
    "\"https://vulpea.blog/category/maternitate/\"\n",
    "]\n",
    "\n",
    "\n",
    "fileName = \"parentingMd.csv\"\n",
    "with open(fileName, 'w'):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#planetamami\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        for garb in soup.find_all(class_='entry-title'):\n",
    "            garb.extract()\n",
    "\n",
    "        div = soup.find('article', class_=\"post\")\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "      \n",
    "      \n",
    "blogIdx = 1\n",
    "url = parentingBlogsMd[blogIdx-1]\n",
    "planetaMami = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 minute time limit exceeded,  210  unsuccesful tries\n"
     ]
    }
   ],
   "source": [
    "#copilariamamicilor\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div', class_=\"entry-content\")\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "      \n",
    "      \n",
    "blogIdx = 2\n",
    "url = parentingBlogsMd[blogIdx-1]\n",
    "copilariaMamicilor = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 20 articles out of 100 for https://jurnaldetatic.blogspot.com/\n"
     ]
    }
   ],
   "source": [
    "#jurnaldetatic\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div', class_=\"entry-content\")\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "      \n",
    "      \n",
    "blogIdx = 3\n",
    "url = parentingBlogsMd[blogIdx-1]\n",
    "jurnaldetatic = extractAll(url,fileName,blogIdx,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 minute time limit exceeded,  440  unsuccesful tries\n"
     ]
    }
   ],
   "source": [
    "#vulpea\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div', class_=\"entry-content\")\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "                \n",
    "\n",
    "blogIdx = 4\n",
    "url = parentingBlogsMd[blogIdx-1]\n",
    "vulpea = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentingDataMd = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "parentingDataMd = pd.concat([parentingDataMd, planetaMami], ignore_index=True) \n",
    "parentingDataMd = pd.concat([parentingDataMd, copilariaMamicilor], ignore_index=True) \n",
    "parentingDataMd = pd.concat([parentingDataMd, jurnaldetatic], ignore_index=True) \n",
    "parentingDataMd = pd.concat([parentingDataMd, vulpea], ignore_index=True) \n",
    "\n",
    "parentingDataMd.to_csv(fileName, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentingBlogsRo = [\n",
    "\"https://www.parentingconstient.ro/blog/\",\n",
    "\"https://suntpitic.ro/\",\n",
    "\"https://www.parentingineradigitala.ro/blog\",\n",
    "\"https://meseriadeparinte.ro/\",\n",
    "\"https://fricidemamici.ro/category/parinteala/\"\n",
    "]\n",
    "\n",
    "fileName = \"parentingRo.csv\"\n",
    "with open(fileName, 'w'):\n",
    "    pass\n",
    "\n",
    "parentingDataRo = pd.DataFrame(columns=['siteIdx','link', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 70 articles out of 100 for https://www.parentingconstient.ro/blog/\n"
     ]
    }
   ],
   "source": [
    "#parenting constient\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        div = soup.find('div', class_=\"entry-content\")\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "                \n",
    "\n",
    "blogIdx = 1\n",
    "url = parentingBlogsRo[blogIdx-1]\n",
    "parenting_constient = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sunt pitic\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        div = soup.find('div', class_=\"entry-content\")\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "                \n",
    "\n",
    "blogIdx = 2\n",
    "url = parentingBlogsRo[blogIdx-1]\n",
    "suntpitic = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 52 articles out of 100 for https://www.parentingineradigitala.ro/blog\n"
     ]
    }
   ],
   "source": [
    "#era digitala\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        div = soup.find('div', class_=\"blog-post-body__content\")\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "                \n",
    "\n",
    "blogIdx = 3\n",
    "url = parentingBlogsRo[blogIdx-1]\n",
    "era_digitala = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mederia_de_parinte\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_=\"the_content\")\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 100 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 4\n",
    "url = parentingBlogsRo[blogIdx-1]\n",
    "mederia_de_parinte = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frici de mamici\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_=\"cm-entry-summary\")\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 100 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 5\n",
    "url = parentingBlogsRo[blogIdx-1]\n",
    "frici_de_mamici = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentingDataRo = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "parentingDataRo = pd.concat([parentingDataRo, parenting_constient], ignore_index=True) \n",
    "parentingDataRo = pd.concat([parentingDataRo, suntpitic], ignore_index=True) \n",
    "parentingDataRo = pd.concat([parentingDataRo, era_digitala], ignore_index=True) \n",
    "parentingDataRo = pd.concat([parentingDataRo, mederia_de_parinte], ignore_index=True) \n",
    "parentingDataRo = pd.concat([parentingDataRo, frici_de_mamici], ignore_index=True) \n",
    "\n",
    "parentingDataRo.to_csv(fileName, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sportBlogsRo = [\n",
    "\"https://www.holokolo.ro/blog/reactor\",\n",
    "\"https://www.humanfitness.ro/blog\",\n",
    "\"https://www.imreandrea.ro/blog/\",\n",
    "\"https://dancefit.ro/blog/\",\n",
    "\"https://almalibre.ro/blog/\",\n",
    "\"https://alexanderflorescu.ro/blog/\",\n",
    "\"https://doinaiosif.ro/blog\",\n",
    "]\n",
    "\n",
    "fileName = \"sportRo.csv\"\n",
    "with open(fileName, 'w'):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 minute time limit exceeded,  23  unsuccesful tries\n"
     ]
    }
   ],
   "source": [
    "#kolokolo\n",
    "import json\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "        \n",
    "        if script_tag:\n",
    "            json_content = json.loads(script_tag.string)\n",
    "            article_body = json_content.get('articleBody', '')\n",
    "            soup = BeautifulSoup(article_body, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            \n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0 and '.' in line)\n",
    "            \n",
    "            if len(text) > 100 and detect(text) == 'ro':\n",
    "                return text\n",
    "            \n",
    "blogIdx = 1\n",
    "url = sportBlogsRo[blogIdx-1]\n",
    "holokolo = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 60 articles out of 100 for https://www.humanfitness.ro/blog\n"
     ]
    }
   ],
   "source": [
    "#human fitness\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_=\"elementor-element-493dc825\")\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 100 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 2\n",
    "url = sportBlogsRo[blogIdx-1]\n",
    "human_fitness = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 34 articles out of 100 for https://www.imreandrea.ro/blog/\n"
     ]
    }
   ],
   "source": [
    "#imreandrea\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_=\"text-content\")\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 100 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 3\n",
    "url = sportBlogsRo[blogIdx-1]\n",
    "imreandrea = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 41 articles out of 100 for https://dancefit.ro/blog/\n"
     ]
    }
   ],
   "source": [
    "#dance fit\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_=\"entry-content\")\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 100 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 4\n",
    "url = sportBlogsRo[blogIdx-1]\n",
    "dance_fit = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 29 articles out of 100 for https://almalibre.ro/blog/\n"
     ]
    }
   ],
   "source": [
    "#alma_libre\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_=\"entry-content\")\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 100 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 5\n",
    "url = sportBlogsRo[blogIdx-1]\n",
    "alma_libre = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 37 articles out of 100 for https://alexanderflorescu.ro/blog/\n"
     ]
    }
   ],
   "source": [
    "#alexander_florescu\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_=\"entry-content\")\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 100 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 6\n",
    "url = sportBlogsRo[blogIdx-1]\n",
    "alexander_florescu = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 56 articles out of 100 for https://doinaiosif.ro/blog\n"
     ]
    }
   ],
   "source": [
    "#doina_iosif\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_=\"elementor-widget-theme-post-content\")\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 7\n",
    "url = sportBlogsRo[blogIdx-1]\n",
    "doina_iosif = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sportDataRo = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "sportDataRo = pd.concat([sportDataRo, holokolo], ignore_index=True) \n",
    "sportDataRo = pd.concat([sportDataRo, human_fitness], ignore_index=True) \n",
    "sportDataRo = pd.concat([sportDataRo, imreandrea], ignore_index=True) \n",
    "sportDataRo = pd.concat([sportDataRo, dance_fit], ignore_index=True) \n",
    "sportDataRo = pd.concat([sportDataRo, alma_libre], ignore_index=True) \n",
    "sportDataRo = pd.concat([sportDataRo, alexander_florescu], ignore_index=True) \n",
    "sportDataRo = pd.concat([sportDataRo, doina_iosif], ignore_index=True) \n",
    "\n",
    "\n",
    "sportDataRo.to_csv(fileName, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sportBlogsMd = [\n",
    "\"https://blog.antrenor.md/\",\n",
    "\"https://sandugrecu.blogspot.com/\",\n",
    "\"https://acvilasport.md/blog\",\n",
    "\"https://dinamo.md/blog/\",\n",
    "\"https://unica.md/sport/\",\n",
    "\"https://www.ellefitness.md/ro/blog\",\n",
    "\"https://fmf.md/blog\"\n",
    "]\n",
    "\n",
    "fileName = \"sportMd.csv\"\n",
    "with open(fileName, 'w'):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 12 articles out of 100 for https://blog.antrenor.md/\n"
     ]
    }
   ],
   "source": [
    "#antrenor\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "\n",
    "        content_div = soup.find_all('p')\n",
    "\n",
    "        if content_div:\n",
    "            extracted_paragraphs = []\n",
    "            \n",
    "            for p_tag in content_div:\n",
    "                paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                extracted_paragraphs.append(paragraph_text)\n",
    "            \n",
    "            text = ' '.join(extracted_paragraphs)\n",
    "            \n",
    "            words = text.split()\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            \n",
    "            if len(text) > 100 and detect(text) == 'ro':\n",
    "                return text\n",
    "                \n",
    "      \n",
    "    \n",
    "blogIdx = 1\n",
    "url = sportBlogsMd[blogIdx-1]\n",
    "antrenor = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilinc\\AppData\\Local\\Temp\\ipykernel_28028\\3977977412.py:20: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(extracted_content, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "#sandu_grecu\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_=\"entry-content\")\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 2\n",
    "url = sportBlogsMd[blogIdx-1]\n",
    "sandu_grecu = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 minute time limit exceeded,  634  unsuccesful tries\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#acvila sport\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "            \n",
    "\n",
    "        content_div = soup.find('div', class_=\"text mt-20-768\")\n",
    "\n",
    "        \n",
    "        if content_div:\n",
    "            extracted_content = content_div.get_text()\n",
    "            soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            # Filter out words containing \"https://\"\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "      \n",
    "    \n",
    "blogIdx = 3\n",
    "url = sportBlogsMd[blogIdx-1]\n",
    "acvila_sport = extractAll(url,fileName,blogIdx,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dinamo\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='main-content')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "blogIdx = 4\n",
    "url = sportBlogsMd[blogIdx-1]\n",
    "dinamo = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#unica\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='article-content')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "blogIdx = 5\n",
    "url = sportBlogsMd[blogIdx-1]\n",
    "unica = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sportDataMd = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "sportDataMd = pd.concat([sportDataMd, antrenor], ignore_index=True) \n",
    "sportDataMd = pd.concat([sportDataMd, sandu_grecu], ignore_index=True) \n",
    "sportDataMd = pd.concat([sportDataMd, acvila_sport], ignore_index=True) \n",
    "sportDataMd = pd.concat([sportDataMd, dinamo], ignore_index=True) \n",
    "sportDataMd = pd.concat([sportDataMd, unica], ignore_index=True) \n",
    "\n",
    "\n",
    "sportDataMd.to_csv(fileName, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelBlogsMd = [\n",
    "\"https://nataalbot.md/category/calatorii/\",\n",
    "\"https://travelblog.md/\",\n",
    "\"http://gurez.md/\",\n",
    "\"https://orheianca.eu/\",\n",
    "\"https://anamariaursublog.wordpress.com/\",\n",
    "\"https://eucalatorul.net/\",\n",
    "\"https://altblogdecalatorii.wordpress.com/\",\n",
    "\"https://wagabond.blog/\"\n",
    "]\n",
    "\n",
    "fileName = \"travelMd.csv\"\n",
    "with open(fileName, 'w'):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nataalbot\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        meta_tag = soup.find('meta', property=\"og:description\")\n",
    "\n",
    "        if meta_tag:\n",
    "            extracted_content = meta_tag['content']\n",
    "            \n",
    "            if extracted_content:\n",
    "                soup = BeautifulSoup(extracted_content, 'html.parser')\n",
    "                text = soup.get_text(separator=\" \", strip=True)\n",
    "                words = text.split()\n",
    "                # Filter out words containing \"https://\"\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "                \n",
    "\n",
    "blogIdx = 1\n",
    "url = travelBlogsMd[blogIdx-1]\n",
    "nataalbot = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#travel blog\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='entry-content')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "blogIdx = 2\n",
    "url = travelBlogsMd[blogIdx-1]\n",
    "travel_blog = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gurez\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='post-entry')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "blogIdx = 3\n",
    "url = travelBlogsMd[blogIdx-1]\n",
    "gurez = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orheianca\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        post_tags_div = soup.find('div', class_='post-tags')\n",
    "        if post_tags_div:\n",
    "            post_tags_div.extract()\n",
    "\n",
    "        div = soup.find('div',id='content-area')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "blogIdx = 4\n",
    "url = travelBlogsMd[blogIdx-1]\n",
    "orheianca = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 54 articles out of 100 for https://anamariaursublog.wordpress.com/\n"
     ]
    }
   ],
   "source": [
    "#ursul\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='entry-content')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "blogIdx = 5\n",
    "url = travelBlogsMd[blogIdx-1]\n",
    "ursul = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelDataMd = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "travelDataMd = pd.concat([travelDataMd, nataalbot], ignore_index=True) \n",
    "travelDataMd = pd.concat([travelDataMd, travel_blog], ignore_index=True) \n",
    "travelDataMd = pd.concat([travelDataMd, gurez], ignore_index=True) \n",
    "travelDataMd = pd.concat([travelDataMd, orheianca], ignore_index=True) \n",
    "travelDataMd = pd.concat([travelDataMd, ursul], ignore_index=True) \n",
    "\n",
    "\n",
    "travelDataMd.to_csv(fileName, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelBlogsRo = [\n",
    "\"https://www.imperatortravel.ro/\",\n",
    "\"https://lipa-lipa.ro/\",\n",
    "\"https://sacalatorim.ro/\",\n",
    "\"https://travelista.ro/\",\n",
    "\"https://blogulmeudecalator.ro/\",\n",
    "\"https://chiperaria.wordpress.com/\",\n",
    "\"https://www.mihaijurca.ro/\"\n",
    "]\n",
    "\n",
    "\n",
    "fileName = \"travelRo.csv\"\n",
    "with open(fileName, 'w'):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imperator travel\n",
    "\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        post_tags_div = soup.find('div', class_=\"comment-respond\")\n",
    "        if post_tags_div:\n",
    "            post_tags_div.extract()\n",
    "\n",
    "\n",
    "        content_div = soup.find_all('p')\n",
    "\n",
    "        if content_div:\n",
    "            extracted_paragraphs = []\n",
    "            \n",
    "            for p_tag in content_div:\n",
    "                paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                extracted_paragraphs.append(paragraph_text)\n",
    "            \n",
    "            text = ' '.join(extracted_paragraphs)\n",
    "            \n",
    "            words = text.split()\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            \n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "    \n",
    "blogIdx = 1\n",
    "url = travelBlogsRo[blogIdx-1]\n",
    "imperator = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lipa-lipa\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='entry-content')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "blogIdx = 2\n",
    "url = travelBlogsMd[blogIdx-1]\n",
    "lipa_lipa = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sa calatorim\n",
    "\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        post_tags_div = soup.find('div', class_=\"tdm-descr\")\n",
    "        if post_tags_div:\n",
    "            post_tags_div.extract()\n",
    "\n",
    "        post_tags_div = soup.find('div', class_=\"comments\")\n",
    "        if post_tags_div:\n",
    "            post_tags_div.extract()\n",
    "\n",
    "        post_tags_div = soup.find('div', class_=\"tds-title\")\n",
    "        if post_tags_div:\n",
    "            post_tags_div.extract()\n",
    "\n",
    "        content_div = soup.find_all('p')\n",
    "\n",
    "        if content_div:\n",
    "            extracted_paragraphs = []\n",
    "            \n",
    "            for p_tag in content_div:\n",
    "                paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                extracted_paragraphs.append(paragraph_text)\n",
    "            \n",
    "            text = ' '.join(extracted_paragraphs)\n",
    "            \n",
    "            words = text.split()\n",
    "            filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            lines = filtered_text.split('\\n')\n",
    "            text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "            \n",
    "            if len(text) > 1000 and detect(text) == 'ro':\n",
    "                return text\n",
    "    \n",
    "blogIdx = 3\n",
    "url = travelBlogsRo[blogIdx-1]\n",
    "sacalatorim = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "# travelista\n",
    "def extractText(url):\n",
    "    res = requests.get(url, verify=False)\n",
    "    if res.status_code == 200:\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.extract()\n",
    "        \n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.extract()\n",
    "\n",
    "        div = soup.find('div',class_='mkdf-post-text-main')\n",
    "        if div:\n",
    "            content_div = div.find_all('p')\n",
    "\n",
    "            if content_div:\n",
    "                extracted_paragraphs = []\n",
    "                \n",
    "                for p_tag in content_div:\n",
    "                    paragraph_text = p_tag.get_text(separator=\" \", strip=True)\n",
    "                    extracted_paragraphs.append(paragraph_text)\n",
    "                \n",
    "                text = ' '.join(extracted_paragraphs)\n",
    "                \n",
    "                words = text.split()\n",
    "                filtered_words = [word for word in words if \"https://\" not in word and \"http://\" not in word]\n",
    "                filtered_text = ' '.join(filtered_words)\n",
    "                lines = filtered_text.split('\\n')\n",
    "                text = '\\n'.join(line.strip() for line in lines if len(line.strip()) > 0  and '.' in line)\n",
    "                \n",
    "                if len(text) > 1000 and detect(text) == 'ro':\n",
    "                    return text\n",
    "      \n",
    "blogIdx = 4\n",
    "url = travelBlogsRo[blogIdx-1]\n",
    "travelista = extractAll(url,fileName,blogIdx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelDataRo = pd.DataFrame(columns=['siteIdx','link', 'text'])\n",
    "travelDataRo = pd.concat([travelDataRo, imperator], ignore_index=True) \n",
    "travelDataRo = pd.concat([travelDataRo, lipa_lipa], ignore_index=True) \n",
    "travelDataRo = pd.concat([travelDataRo, sacalatorim], ignore_index=True) \n",
    "travelDataRo = pd.concat([travelDataRo, travelista], ignore_index=True) \n",
    "\n",
    "\n",
    "travelDataRo.to_csv(fileName, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
